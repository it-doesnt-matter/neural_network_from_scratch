# Goal
The goal of this project was to gain an understanding of neural networks. I started by reading two books about the topic (see the first two sources), which helped me to understand the theory (mostly math) behind neural networks. I then wrote my own implementation to deepen my comprehension of the subject.

Since this project is purely for educational purposes and is not intended for further use, the code quality is not at production level (missing exception handling, no documentation, no tests, ...).

# Possible Future Additions
- convolutional and pooling layers
- time-based learning rate decay
- learning rate decay with early stopping
- batch-size increase instead of learning rate decay (arXiv:1711.00489)
- L1 and L2 regularization
- SGD with momentum
- AdaGrad
- RMSprop

# Sources
- Neural Networks from Scratch in Python by Harrison Kinsley & Daniel Kukie≈Ça
	- https://nnfs.io
	- https://github.com/Sentdex/nnfs_book
- Neural Networks and Deep Learning by Michael Nielsen
	- http://neuralnetworksanddeeplearning.com
	- https://github.com/mnielsen/neural-networks-and-deep-learning
- https://cs231n.github.io/neural-networks-case-study (only for the data generating code)
- http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking
- https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Gradient%20Checking.ipynb
- https://github.com/gallettilance/kviz (for the code for drawing the decision boundaries)
